using CSV, DataFrames, JSON3
using Statistics, StatsBase
using Printf
using DataStructures: OrderedDict

# Define functions to compute correlations and boostrap CI intervals

"Returns the concordance correlation coefficient (CCC) for two vectors."
function ccc(x, y; corrected::Bool=true)
    @assert length(x) == length(y)
    s_xy = cov(x, y; corrected)
    var_x = var(x; corrected)
    var_y = var(y; corrected)
    m_x = mean(x)
    m_y = mean(y)
    return 2 * s_xy / (var_x + var_y + (m_x - m_y)^2)
end

"Returns the mean absolute error between two vectors."
mae(x, y) = mean(abs.(x .- y))

"""
    sim_with(f::Function, x::AbstractMatrix)

Returns a function `sim_with_x(cols...)` that computes similarity of a
set of vectors `cols` according to a function `f` with an input matrix `x`.
"""
function sim_with(f::Function, x::AbstractMatrix)
    sim_with_x(y::AbstractMatrix) = f(vec(y), vec(x))
    sim_with_x(cols::AbstractVector...) = sim_with_x(stack(cols))
    return sim_with_x
end

"""
    sim_ci_with(f::Function, xs::Vector)

Returns a function `sim_ci_with_xs(cols...)` that computes a 95% confidence
interval for the similarity of a set of vectors `cols`, where similarity is
computed according to a function `f` against each element `x` of a set
of samples `xs`.
"""
function sim_ci_with(f::Function, xs::Vector)
    function sim_ci_with_xs(y::AbstractMatrix)
        all_sims = map(x -> f(vec(y), vec(x)), xs)
        if any(ismissing.(all_sims))
            return [missing missing]
        elseif any(isnan.(all_sims))
            return [NaN NaN]
        else
            return quantile(all_sims, [0.025, 0.975])'
        end
    end
    sim_ci_with_xs(cols::AbstractVector...) = sim_ci_with_xs(stack(cols))
    return sim_ci_with_xs
end

"""
    sim_se_with(f::Function, xs::Vector)

Returns a function `sim_se_with_xs(cols...)` that computes the standard
error of the similarity of a set of vectors `cols`, where similarity is
computed according to a function `f` against each element `x` of a set
of samples `xs`.
"""
function sim_se_with(f::Function, xs::Vector)
    function sim_se_with_xs(y::AbstractMatrix)
        all_sims = map(x -> f(vec(y), vec(x)), xs)
        if any(ismissing.(all_sims))
            return missing
        else
            return std(all_sims)
        end
    end
    sim_se_with_xs(cols::AbstractVector...) = sim_se_with_xs(stack(cols))
    return sim_se_with_xs
end

EXPERIMENT_ID = "exp2_current"
RESULTS_DIR = joinpath(@__DIR__, "results")
HUMAN_RESULTS_DIR = joinpath(RESULTS_DIR, "humans", EXPERIMENT_ID)
STIMULI_JSON_PATH = joinpath(@__DIR__, "dataset/stimuli/stimuli.json")

## Preprocess human data

# Load firebase data
firebase_path = joinpath(HUMAN_RESULTS_DIR, "firebase_data.json")
firebase_data = JSON3.read(read(firebase_path, String))

# Load Qualtrics data
qualtrics_path = joinpath(HUMAN_RESULTS_DIR, "qualtrics_data.csv")
qualtrics_data = CSV.read(qualtrics_path, DataFrame, header=2, skipto=4)

# Extract experiment completion codes generated by Firebase app
participant_codes = qualtrics_data[:, "Experiment-Code"]

# Extract Firebase entries and entry names
entries = firebase_data.results
entry_names = collect(keys(firebase_data.results))

# Extract global participant data
participant_df = DataFrame()
for code in participant_codes
    row = Dict()
    entry = entries[Symbol(code)]
    row[:participant_code] = code
    # Add stimuli set
    row[:stimuli_set] = entry.stimuli_set
    # Add exam score
    row[:exam_results] = entry.exam.results
    row[:exam_score] = entry.exam.score
    # Add total reward
    row[:total_reward] = parse(Float64, entry.total_reward)
    # Add total payment
    row[:total_payment] = parse(Float64, entry.total_payment)
    push!(participant_df, row; cols=:union)
end

# Add Prolific IDs
prolific_id = qualtrics_data[:, "Prolific-ID"]
prolific_id =participant_codes
participant_df.prolific_id = prolific_id

# Print bonus for bulk payment on Prolific
for row in eachrow(participant_df)
    payment = row.total_payment/2
    println(row.prolific_id, ", ", round(payment, digits=2))
end

# Write participant data to CSV
CSV.write(joinpath(HUMAN_RESULTS_DIR, "participant_data.csv"), participant_df)

# Filter out outlier participants
q1 = quantile(participant_df.total_payment, 0.25)
outlier_threshold = q1 - 1.5 * iqr(participant_df.total_payment)
participant_codes = filter(r -> r.total_payment >= outlier_threshold, participant_df).participant_code

# Extract per-participant stimulus responses
human_df = DataFrame(
    participant_code = Int[],
    plan_id = String[],
    step = Int[],
    timestep = Int[],
    goal_probs_1 = Float64[],
    goal_probs_2 = Float64[],
    goal_probs_3 = Float64[],
    goal_probs_4 = Float64[],
    true_goal_probs = Float64[],
    statement_ratings_1 = Float64[],
    statement_ratings_2 = Float64[],
    statement_ratings_3 = Float64[],
    statement_ratings_4 = Float64[],
    statement_ratings_5 = Float64[],
    statement_probs_1 = Float64[],
    statement_probs_2 = Float64[],
    statement_probs_3 = Float64[],
    statement_probs_4 = Float64[],
    statement_probs_5 = Float64[],
    reward = Float64[],
    goal_reward = Float64[],
    time_spent = Float64[],
)
for code in participant_codes
    participant_entry = entries[Symbol(code)]
    for (name, stim_entry) in pairs(participant_entry)
        # Extract only entries of the form X_X
        m = match(r"(\d+)_(\d+)", string(name))
        isnothing(m) && continue
        for (step_idx, step_entry) in stim_entry
            step_idx == :reward && continue
            row = Dict{Symbol, Any}()
            for (key, val) in step_entry
                if val isa AbstractArray
                    for (idx, v) in enumerate(val)
                        row[Symbol(key, "_", idx)] = v
                    end
                elseif val isa Real
                    row[key] = val
                else
                    row[key] = parse(Float64, val)
                end
            end
            row[:participant_code] = code
            row[:plan_id] = name
            row[:step] = parse(Int, string(step_idx)) + 1
            push!(human_df, row; cols=:union)
        end
    end
end
sort!(human_df, [:participant_code, :plan_id, :timestep])

# Replace -1 entries with missing
transform!(human_df,
    ["statement_probs_$i" => (x -> replace(x, -1.0 => missing)) => "statement_probs_$i" for i in 1:5]
)

# Write per-participant data to CSV
CSV.write(joinpath(HUMAN_RESULTS_DIR, "human_data.csv"), human_df)

# Average data for each stimulus
group_df = DataFrames.groupby(human_df, [:plan_id, :timestep])
mean_human_df = combine(group_df,
    ["goal_probs_$i" => mean => "goal_probs_$i" for i in 1:4]...,
    ["goal_probs_$i" => std => "goal_probs_std_$i" for i in 1:4]...,
    ["goal_probs_$i" => sem => "goal_probs_sem_$i" for i in 1:4]...,
    "true_goal_probs" => mean => "true_goal_probs",
    "true_goal_probs" => std => "true_goal_probs_std",
    ["statement_probs_$i" => (x -> mean(skipmissing(x))) => "statement_probs_$i" for i in 1:5]...,
    ["statement_probs_$i" => (x -> std(skipmissing(x))) => "statement_probs_std_$i" for i in 1:5]...,
    ["statement_probs_$i" => (x -> sem(skipmissing(x))) => "statement_probs_sem_$i" for i in 1:5]...,
    :goal_reward => mean => :goal_reward,
    :goal_reward => std => :goal_reward_std,
    :reward => mean => :reward,
    :reward => std => :reward_std,
)
sort!(mean_human_df, [:plan_id, :timestep])

# Write aggregated data per stimulus to CSV
CSV.write(joinpath(HUMAN_RESULTS_DIR, "mean_human_data.csv"), mean_human_df)

## Comparative analysis

RESULTS_DIR = joinpath(@__DIR__, "results")
CURR_HUMAN_DIR = joinpath(@__DIR__, "results", "humans", "exp2_current")
INIT_HUMAN_DIR = joinpath(@__DIR__, "results", "humans", "exp2_initial")

START_COLS = [:method, :submethod, :plan_id, :judgment_id, :belief_type]

# Read human data
df_path = "mean_human_data.csv"
df_path = joinpath(CURR_HUMAN_DIR, df_path)
curr_human_df = CSV.read(df_path, DataFrame)
sort!(curr_human_df, [:plan_id, :timestep])
curr_human_df.method .= "human"
curr_human_df.submethod .= ""
curr_human_df.belief_type .= "current"

df_path = "mean_human_data.csv"
df_path = joinpath(INIT_HUMAN_DIR, df_path)
init_human_df = CSV.read(df_path, DataFrame)
sort!(init_human_df, [:plan_id, :timestep])
init_human_df.method .= "human"
init_human_df.submethod .= ""
init_human_df.belief_type .= "initial"

human_df = vcat(curr_human_df, init_human_df, cols=:union)
human_df.judgment_id = 1:size(human_df, 1)
human_df.timestep .-= 1
select!(human_df, START_COLS, Not(START_COLS))

# Write combined human data
CSV.write(joinpath(RESULTS_DIR, "results_human_mean.csv"), human_df)

# Read data for the BToM models
df_path = "results_btom_all.csv"
df_path = joinpath(RESULTS_DIR, df_path)
btom_df = CSV.read(df_path, DataFrame)
filter!(r -> r.is_judgment, btom_df)
sort!(btom_df,
    [:belief_type, :submethod,
     :goal_prior, :state_prior, :belief_prior,
     :policy_type, :value_function, :act_temperature,
     :plan_id, :timestep]
)
btom_df.method .= "btom"
gdf = DataFrames.groupby(btom_df,
    [:submethod, :goal_prior, :state_prior, :belief_prior,
     :policy_type, :value_function, :act_temperature]
)
for group in gdf
    group.judgment_id = 1:size(group, 1)
end
select!(btom_df, START_COLS, Not(START_COLS))

# Extract true goal IDs
true_goals = gdf[1].true_goal
true_goals_curr = true_goals[1:size(curr_human_df, 1)]
true_goals_init = true_goals[size(curr_human_df, 1)+1:end]

# Read data for LLM baselines
LLM_RESULTS_DIR = joinpath(@__DIR__, "results", "llm_baselines")
LLM_DF_PATHS = [
    ("gpt-4o", "image, plan", "current") => "gpt4o_exp2_current_plan.csv",
    ("gpt-4o", "image, plan", "initial") => "gpt4o_exp2_initial_plan.csv",
    ("gpt-4o", "image, narrative", "current") => "gpt4o_exp2_current_narrative.csv",
    ("gpt-4o", "image, narrative", "initial") => "gpt4o_exp2_initial_narrative.csv",
    ("gpt-4o", "image, narrative, few-shot", "current") => "gpt4o_exp2_current_narrative_few_shot.csv",
    ("gpt-4o", "image, narrative, few-shot", "initial") => "gpt4o_exp2_initial_narrative_few_shot.csv",
    ("gemini-pro", "image, narrative, few-shot", "current") => "gemini_exp2_current_narrative_image_few_shot.csv",
    ("gemini-pro", "image, narrative, few-shot", "initial") => "gemini_exp2_initial_narrative_image_few_shot.csv",
    ("gemini-pro", "video, narrative, few-shot", "current") => "gemini_exp2_current_narrative_video_few_shot.csv",
    ("gemini-pro", "video, narrative, few-shot", "initial") => "gemini_exp2_initial_narrative_video_few_shot.csv"
]
GOAL_NAMES = ["triangle", "square", "hexagon", "circle"]

llm_df = DataFrame()
for ((method, submethod, belief_type), df_path) in LLM_DF_PATHS
    df_path = joinpath(LLM_RESULTS_DIR, df_path)
    df = CSV.read(df_path, DataFrame)
    # Transform goal inferences
    true_goal_ids = belief_type == "current" ? true_goals_curr : true_goals_init
    goal_ids = [[findfirst(==(strip(g, ['"', ' '])), GOAL_NAMES) for g in split(str, ", ")]
                for str in df.goal]
    goal_probs = [[g in g_ids ? 1.0 : 0.0 for g in 1:4] for g_ids in goal_ids]
    goal_probs = [probs./sum(probs) for probs in goal_probs]
    df.true_goal_probs = [probs[id] for (probs, id) in zip(goal_probs, true_goal_ids)]
    goal_probs = reduce(hcat, goal_probs)
    for i in 1:4
        df[!, "goal_probs_$i"] = goal_probs[i, :]
    end
    # Transform statement ratings
    if eltype(df.statement_rating_1) == Int
        transform!(df,
            ["statement_rating_$i" => (x -> (x.-1)./(7-1)) => "statement_probs_$i" for i in 1:5]...,
            ["statement_rating_$i" => (x -> (x.-1)./(7-1)) => "norm_statement_probs_$i" for i in 1:5]...,
        )
    else
        transform!(df,
            ["statement_rating_$i" => "statement_probs_$i" for i in 1:5]...,
            ["statement_rating_$i" => "norm_statement_probs_$i" for i in 1:5]...,
        )
    end
    select!(df, Not("goal"))
    select!(df, Not(r"^statement_rating_\d+$"))
    sort!(df, [:plan_id, :timestep])
    df.method .= method
    df.submethod .= submethod
    df.belief_type .= belief_type
    append!(llm_df, df, cols=:union)
end

gdf = DataFrames.groupby(llm_df, [:method, :submethod])
for group in gdf
    group.judgment_id = 1:size(group, 1)
end
select!(llm_df, START_COLS, Not(START_COLS))

# Write combined LLM data
CSV.write(joinpath(RESULTS_DIR, "results_llm_all.csv"), llm_df)
llm_df = CSV.read(joinpath(RESULTS_DIR, "results_llm_all.csv"), DataFrame)

# Combine human and model data
combined_df = vcat(btom_df, llm_df, human_df, cols=:union)

# Compute aggregate statistics across dataset
gdf = DataFrames.groupby(combined_df,
    [:belief_type, :method, :submethod,
    :goal_prior, :state_prior, :belief_prior,
    :policy_type, :value_function, :act_temperature]
)
performance_df = combine(gdf,
    :true_goal_probs => mean => :true_goal_probs,
    :true_goal_probs => std => :true_goal_probs_std,
    :true_goal_probs => sem => :true_goal_probs_sem,
)
CSV.write(joinpath(RESULTS_DIR, "performance_results.csv"), performance_df)

## Correlational analysis

get_goal_probs(df) = df[:, ["goal_probs_$i" for i in 1:4]] |> Matrix
get_statement_probs(df) = df[:, ["statement_probs_$i" for i in 1:5]] |> Matrix
get_norm_statement_probs(df) = df[:, ["norm_statement_probs_$i" for i in 1:5]] |> Matrix

function bootstrap_sample(df, row_count, N = 1000)
    sampled_goal_probs = Vector{Matrix{Float64}}()
    sampled_statement_probs = Vector{Matrix{Float64}}()
    participant_ids = unique(df.participant_code)
    for _ in 1:1000
        sampled_ids = sample(participant_ids, length(participant_ids), replace=true)
        tmp_df = filter(r -> r.participant_code in sampled_ids, df)
        tmp_gdf = DataFrames.groupby(tmp_df, [:plan_id, :timestep])
        tmp_mean_df = combine(tmp_gdf,
            ["goal_probs_$i" => mean => "goal_probs_$i" for i in 1:4]...,
            ["statement_probs_$i" => (x -> mean(skipmissing(x))) => "statement_probs_$i" for i in 1:5]...
        )
        if size(tmp_mean_df, 1) != row_count
            continue
        end
        sort!(tmp_mean_df, [:plan_id, :timestep])
        push!(sampled_goal_probs, get_goal_probs(tmp_mean_df))
        push!(sampled_statement_probs, get_statement_probs(tmp_mean_df))
    end
    return sampled_goal_probs, sampled_statement_probs
end

# Extract mean human goal and assist probs
mean_human_goal_probs = get_goal_probs(human_df)
mean_human_statement_probs = get_statement_probs(human_df)

# Compute bootstrap samples of human goal probs
df_path = joinpath(CURR_HUMAN_DIR, "human_data.csv")
all_human_curr_df = CSV.read(df_path, DataFrame)
sampled_goal_probs_curr, sampled_statement_probs_curr =
    bootstrap_sample(all_human_curr_df, size(curr_human_df, 1), 1000)

df_path = joinpath(INIT_HUMAN_DIR, "human_data.csv")
all_human_init_df = CSV.read(df_path, DataFrame)
sampled_goal_probs_init, sampled_statement_probs_init =
    bootstrap_sample(all_human_init_df, size(init_human_df, 1), 1000)

sampled_goal_probs =
    [vcat(p1, p2) for (p1, p2) in zip(sampled_goal_probs_curr, sampled_goal_probs_init)]
sampled_statement_probs =
    [vcat(p1, p2) for (p1, p2) in zip(sampled_statement_probs_curr, sampled_statement_probs_init)]

# Compute correlation with each baseline
model_df = vcat(btom_df, llm_df, cols=:union)

correlation_df = DataFrame(
    belief_type = String[],
    method = String[],
    submethod = String[],
    goal_prior = Float64[],
    state_prior = Float64[],
    belief_prior = Float64[],
    policy_type = String[],
    value_function = String[],
    act_temperature = Float64[],
    goal_cor = Float64[],
    statement_cor = Float64[],
    norm_statement_cor = Float64[],
    statement_ccc = Float64[],
    norm_statement_ccc = Float64[],
    statement_mae = Float64[],
    norm_statement_mae = Float64[],
    goal_cor_se = Float64[],
    statement_cor_se = Float64[],
    norm_statement_cor_se = Float64[],
    goal_cor_ci_lo = Float64[],
    goal_cor_ci_hi = Float64[],
    statement_cor_ci_lo = Float64[],
    statement_cor_ci_hi = Float64[],
    norm_statement_cor_ci_lo = Float64[],
    norm_statement_cor_ci_hi = Float64[],
    statement_mae_se = Float64[],
    norm_statement_mae_se = Float64[],
    statement_mae_ci_lo = Float64[],
    statement_mae_ci_hi = Float64[],
    norm_statement_mae_ci_lo = Float64[],
    norm_statement_mae_ci_hi = Float64[],
)
gdf = DataFrames.groupby(model_df,
    [:belief_type, :method, :submethod,
     :goal_prior, :state_prior, :belief_prior,
     :policy_type, :value_function, :act_temperature]
)
overall_gdf = DataFrames.groupby(model_df,
    [:method, :submethod,
    :goal_prior, :state_prior, :belief_prior,
    :policy_type, :value_function, :act_temperature]
)
for (key, group) in Iterators.flatten((pairs(gdf), pairs(overall_gdf)))
    h_goal_probs = mean_human_goal_probs[group.judgment_id, :]
    h_statement_probs = mean_human_statement_probs[group.judgment_id, :]
    goal_probs = group[:, r"^goal_probs_\d+$"] |> Matrix
    statement_probs = group[:, r"^statement_probs_\d+$"] |> Matrix
    norm_statement_probs = group[:, r"^norm_statement_probs_\d+$"] |> Matrix
    row = OrderedDict{Symbol, Any}(pairs(key))
    if !haskey(row, :belief_type)
        row[:belief_type] = "overall"
    end
    row[:goal_cor] = sim_with(cor, h_goal_probs)(goal_probs)
    row[:statement_cor] = sim_with(cor, h_statement_probs)(statement_probs)
    row[:norm_statement_cor] = sim_with(cor, h_statement_probs)(norm_statement_probs)
    row[:statement_ccc] = sim_with(ccc, h_statement_probs)(statement_probs)
    row[:norm_statement_ccc] = sim_with(ccc, h_statement_probs)(norm_statement_probs)
    row[:statement_mae] = sim_with(mae, h_statement_probs)(statement_probs)
    row[:norm_statement_mae] = sim_with(mae, h_statement_probs)(norm_statement_probs)
    h_sampled_goal_probs = [probs[group.judgment_id, :] for probs in sampled_goal_probs]
    h_sampled_statement_probs = [probs[group.judgment_id, :] for probs in sampled_statement_probs]
    row[:goal_cor_se] = sim_se_with(cor, h_sampled_goal_probs)(goal_probs)
    row[:statement_cor_se] = sim_se_with(cor, h_sampled_statement_probs)(statement_probs)
    row[:norm_statement_cor_se] = sim_se_with(cor, h_sampled_statement_probs)(norm_statement_probs)
    row[:goal_cor_ci_lo], row[:goal_cor_ci_hi] = sim_ci_with(cor, h_sampled_goal_probs)(goal_probs)
    row[:statement_cor_ci_lo], row[:statement_cor_ci_hi] = sim_ci_with(cor, h_sampled_statement_probs)(statement_probs)
    row[:norm_statement_cor_ci_lo], row[:norm_statement_cor_ci_hi] = sim_ci_with(cor, h_sampled_statement_probs)(norm_statement_probs)
    row[:statement_mae_se] = sim_se_with(mae, h_sampled_statement_probs)(statement_probs)
    row[:norm_statement_mae_se] = sim_se_with(mae, h_sampled_statement_probs)(norm_statement_probs)
    row[:statement_mae_ci_lo], row[:statement_mae_ci_hi] = sim_ci_with(mae, h_sampled_statement_probs)(statement_probs)
    row[:norm_statement_mae_ci_lo], row[:norm_statement_mae_ci_hi] = sim_ci_with(mae, h_sampled_statement_probs)(norm_statement_probs)
    push!(correlation_df, row, cols=:union)
end
CSV.write(joinpath(RESULTS_DIR, "human_model_corr.csv"), correlation_df)

# Compute correlation per plan
correlation_per_plan_df = DataFrame(
    belief_type = String[],
    method = String[],
    submethod = String[],
    goal_prior = Float64[],
    state_prior = Float64[],
    belief_prior = Float64[],
    policy_type = String[],
    value_function = String[],
    act_temperature = Float64[],
    plan_id = String[],
    goal_cor = Float64[],
    statement_cor = Float64[],
    norm_statement_cor = Float64[],
    statement_ccc = Float64[],
    norm_statement_ccc = Float64[],
    statement_mae = Float64[],
    norm_statement_mae = Float64[],
    goal_cor_se = Float64[],
    statement_cor_se = Float64[],
    norm_statement_cor_se = Float64[],
    goal_cor_ci_lo = Float64[],
    goal_cor_ci_hi = Float64[],
    statement_cor_ci_lo = Float64[],
    statement_cor_ci_hi = Float64[],
    norm_statement_cor_ci_lo = Float64[],
    norm_statement_cor_ci_hi = Float64[],
    statement_mae_se = Float64[],
    norm_statement_mae_se = Float64[],
    statement_mae_ci_lo = Float64[],
    statement_mae_ci_hi = Float64[],
    norm_statement_mae_ci_lo = Float64[],
    norm_statement_mae_ci_hi = Float64[],
)
gdf = DataFrames.groupby(btom_df,
    [:belief_type, :method, :submethod,
     :goal_prior, :state_prior, :belief_prior,
     :policy_type, :value_function, :act_temperature,
     :plan_id]
)
for (key, group) in pairs(gdf)
    h_goal_probs = mean_human_goal_probs[group.judgment_id, :]
    h_statement_probs = mean_human_statement_probs[group.judgment_id, :]
    goal_probs = group[:, r"^goal_probs_\d+$"] |> Matrix
    statement_probs = group[:, r"^statement_probs_\d+$"] |> Matrix
    norm_statement_probs = group[:, r"^norm_statement_probs_\d+$"] |> Matrix
    row = OrderedDict{Symbol, Any}(pairs(key))
    row[:goal_cor] = sim_with(cor, h_goal_probs)(goal_probs)
    row[:statement_cor] = sim_with(cor, h_statement_probs)(statement_probs)
    row[:norm_statement_cor] = sim_with(cor, h_statement_probs)(norm_statement_probs)
    row[:statement_ccc] = sim_with(ccc, h_statement_probs)(statement_probs)
    row[:norm_statement_ccc] = sim_with(ccc, h_statement_probs)(norm_statement_probs)
    row[:statement_mae] = sim_with(mae, h_statement_probs)(statement_probs)
    row[:norm_statement_mae] = sim_with(mae, h_statement_probs)(norm_statement_probs)
    h_sampled_goal_probs = [probs[group.judgment_id, :] for probs in sampled_goal_probs]
    h_sampled_statement_probs = [probs[group.judgment_id, :] for probs in sampled_statement_probs]
    row[:goal_cor_se] = sim_se_with(cor, h_sampled_goal_probs)(goal_probs)
    row[:statement_cor_se] = sim_se_with(cor, h_sampled_statement_probs)(statement_probs)
    row[:norm_statement_cor_se] = sim_se_with(cor, h_sampled_statement_probs)(norm_statement_probs)
    row[:goal_cor_ci_lo], row[:goal_cor_ci_hi] = sim_ci_with(cor, h_sampled_goal_probs)(goal_probs)
    row[:statement_cor_ci_lo], row[:statement_cor_ci_hi] = sim_ci_with(cor, h_sampled_statement_probs)(statement_probs)
    row[:norm_statement_cor_ci_lo], row[:norm_statement_cor_ci_hi] = sim_ci_with(cor, h_sampled_statement_probs)(norm_statement_probs)
    row[:statement_mae_se] = sim_se_with(mae, h_sampled_statement_probs)(statement_probs)
    row[:norm_statement_mae_se] = sim_se_with(mae, h_sampled_statement_probs)(norm_statement_probs)
    row[:statement_mae_ci_lo], row[:statement_mae_ci_hi] = sim_ci_with(mae, h_sampled_statement_probs)(statement_probs)
    row[:norm_statement_mae_ci_lo], row[:norm_statement_mae_ci_hi] = sim_ci_with(mae, h_sampled_statement_probs)(norm_statement_probs)
    push!(correlation_per_plan_df, row, cols=:union)
end
CSV.write(joinpath(RESULTS_DIR, "human_model_corr_per_plan.csv"), correlation_per_plan_df)

# Compute correlation per statement
include("src/plan_io.jl")
CURR_STATEMENT_DIR = joinpath(@__DIR__, "dataset", "statements", "exp2_current")
INIT_STATEMENT_DIR = joinpath(@__DIR__, "dataset", "statements", "exp2_initial")
_, CURR_STATEMENTS = load_statement_dataset(CURR_STATEMENT_DIR)
_, INIT_STATEMENTS = load_statement_dataset(INIT_STATEMENT_DIR)
correlation_per_statement_df = DataFrame(
    belief_type = String[],
    method = String[],
    submethod = String[],
    goal_prior = Float64[],
    state_prior = Float64[],
    belief_prior = Float64[],
    policy_type = String[],
    value_function = String[],
    act_temperature = Float64[],
    plan_id = String[],
    statement_id = Int[],
    statement = String[],
    statement_cor = Float64[],
    norm_statement_cor = Float64[],
    statement_ccc = Float64[],
    norm_statement_ccc = Float64[],
    statement_mae = Float64[],
    norm_statement_mae = Float64[],
)
gdf = DataFrames.groupby(btom_df,
    [:belief_type, :method, :submethod,
     :goal_prior, :state_prior, :belief_prior,
     :policy_type, :value_function, :act_temperature,
     :plan_id]
)
for (key, group) in pairs(gdf)
    statements = key.belief_type == "initial" ?
        INIT_STATEMENTS[String(key.plan_id)] : CURR_STATEMENTS[String(key.plan_id)]
    for id in 1:5
        h_statement_probs = mean_human_statement_probs[group.judgment_id, id]
        statement_probs = group[:, "statement_probs_$id"]
        norm_statement_probs = group[:, "norm_statement_probs_$id"]
        row = OrderedDict{Symbol, Any}(pairs(key))
        row[:statement_id] = id
        row[:statement] = statements[id]
        row[:statement_cor] = cor(statement_probs, h_statement_probs)
        row[:norm_statement_cor] = cor(norm_statement_probs, h_statement_probs)
        row[:statement_ccc] = ccc(statement_probs, h_statement_probs)
        row[:norm_statement_ccc] = ccc(norm_statement_probs, h_statement_probs)
        row[:statement_mae] = mean(abs.(statement_probs - h_statement_probs))
        row[:norm_statement_mae] = mean(abs.(norm_statement_probs - h_statement_probs))
        push!(correlation_per_statement_df, row, cols=:union)
    end
end
CSV.write(joinpath(RESULTS_DIR, "human_model_corr_per_statement.csv"), correlation_per_statement_df)

## Per-factor correlation analysis

STATEMENT_DIR = joinpath(@__DIR__, "dataset", "statements")

df_path = joinpath(STATEMENT_DIR, "exp2_current_annotations.csv")
curr_annotations_df = CSV.read(df_path, DataFrame)
curr_annotations_df.belief_type .= "current"

df_path = joinpath(STATEMENT_DIR, "exp2_initial_annotations.csv")
init_annotations_df = CSV.read(df_path, DataFrame)
init_annotations_df.belief_type .= "initial"

annotations_df = vcat(curr_annotations_df, init_annotations_df, cols=:union)

# Load human data
df_path = joinpath(RESULTS_DIR, "results_human_mean.csv")
human_df = CSV.read(df_path, DataFrame)
select!(human_df, Not(r"goal_probs", r"goal_reward", r"reward"))

# Stack human data so that each statement judgment has its own row
stacked_df = stack(human_df,
    [["statement_probs_$i" for i in 1:5]; ["statement_probs_std_$i" for i in 1:5]; ["statement_probs_sem_$i" for i in 1:5]]
)
transform!(stacked_df,
    :variable => (xs -> parse.(Int, last.(xs))) => :statement_id,
    :variable => (xs -> [x[1:end-2] for x in xs]) => :variable
)
human_statements_df = unstack(stacked_df)
sort!(human_statements_df, [:judgment_id, :statement_id])

# Join human data with annotations
human_statements_df = innerjoin(
    human_statements_df, annotations_df,
    on=[:plan_id, :statement_id, :belief_type]
)
CSV.write(joinpath(RESULTS_DIR, "statement_results_human.csv"), human_statements_df)

# Load BToM data
df_path = joinpath(RESULTS_DIR, "results_btom_all.csv")
btom_df = CSV.read(df_path, DataFrame)
btom_df.method .= "btom"
filter!(r -> r.is_judgment, btom_df)
filter!(r -> r.act_temperature == 0.354, btom_df)
sort!(btom_df, [:submethod, :belief_type, :plan_id, :timestep])

gdf = DataFrames.groupby(btom_df, [:submethod])
for group in gdf
    group.judgment_id = 1:size(group, 1)
end
select!(btom_df, START_COLS, Not(START_COLS))
select!(btom_df, Not(r"goal_probs", r"^statement_probs_(\d+)$"))

# Stack BToM data so that each statement judgment has its own row
stacked_df = stack(btom_df, ["norm_statement_probs_$i" for i in 1:5])
transform!(stacked_df,
    :variable => (xs -> parse.(Int, last.(xs))) => :statement_id,
    :variable => (xs -> [x[1:end-2] for x in xs]) => :variable
)
btom_statements_df = unstack(stacked_df)
rename!(btom_statements_df, :norm_statement_probs => :statement_probs)
sort!(btom_statements_df, [:method, :submethod, :judgment_id, :statement_id])

# Join BToM data with annotations
btom_statements_df = innerjoin(
    btom_statements_df, annotations_df,
    on=[:plan_id, :statement_id, :belief_type]
)
CSV.write(joinpath(RESULTS_DIR, "statement_results_btom.csv"), btom_statements_df)

# Load LLM data
df_path = joinpath(RESULTS_DIR, "results_llm_all.csv")
llm_df = CSV.read(df_path, DataFrame)
sort!(llm_df, [:submethod, :belief_type, :plan_id, :timestep])
select!(llm_df, Not(r"goal_probs", r"^norm_statement_probs_(\d+)$"))

# Stack LLM data so that each statement judgment has its own row
stacked_df = stack(llm_df, ["statement_probs_$i" for i in 1:5])
transform!(stacked_df,
    :variable => (xs -> parse.(Int, last.(xs))) => :statement_id,
    :variable => (xs -> [x[1:end-2] for x in xs]) => :variable
)
llm_statements_df = unstack(stacked_df)
sort!(llm_statements_df, [:method, :submethod, :judgment_id, :statement_id])

# Join LLM data with annotations
llm_statements_df = innerjoin(
    llm_statements_df, annotations_df,
    on=[:plan_id, :statement_id, :belief_type]
)
CSV.write(joinpath(RESULTS_DIR, "statement_results_llm.csv"), llm_statements_df)

# Combine all model statement ratings data
statements_df = vcat(btom_statements_df, llm_statements_df, cols=:union)
statements_df.possibility = statements_df.possibility .!= "none"
statements_df.probability = statements_df.probability .!= "none"

sort!(statements_df, [:method, :submethod, :judgment_id, :statement_id])
gdf = DataFrames.groupby(statements_df, [:method, :submethod])
for group in gdf
    group.rating_id = 1:size(group, 1)
end

statement_correlation_df = DataFrame(
    method = String[],
    submethod = String[],
    all_cor = Float64[],
    all_possibility_cor = Float64[],
    all_probability_cor = Float64[],
    all_compositional_cor = Float64[],
    all_knowledge_cor = Float64[],
    curr_cor = Float64[],
    curr_possibility_cor = Float64[],
    curr_probability_cor = Float64[],
    curr_compositional_cor = Float64[],
    curr_knowledge_cor = Float64[],
    init_cor = Float64[],
    init_possibility_cor = Float64[],
    init_probability_cor = Float64[],
    init_compositional_cor = Float64[],
    init_knowledge_cor = Float64[],
    all_cor_se = Float64[],
    all_cor_ci_lo = Float64[],
    all_cor_ci_hi = Float64[],
    all_possibility_cor_se = Float64[],
    all_probability_cor_se = Float64[],
    all_compositional_cor_se = Float64[],
    all_knowledge_cor_se = Float64[],
    curr_cor_se = Float64[],
    curr_cor_ci_lo = Float64[],
    curr_cor_ci_hi = Float64[],
    curr_possibility_cor_se = Float64[],
    curr_probability_cor_se = Float64[],
    curr_compositional_cor_se = Float64[],
    curr_knowledge_cor_se = Float64[],
    init_cor_se = Float64[],
    init_cor_ci_lo = Float64[],
    init_cor_ci_hi = Float64[],
    init_possibility_cor_se = Float64[],
    init_probability_cor_se = Float64[],
    init_compositional_cor_se = Float64[],
    init_knowledge_cor_se = Float64[]
)

conditions = [
    "all" => ["current", "initial"],
    "curr" => ["current"],
    "init" => ["initial"]
]
factors = ["possibility", "probability", "compositional", "knowledge"]

gdf = DataFrames.groupby(statements_df, [:method, :submethod])
for (key, group) in pairs(gdf)
    row = OrderedDict{Symbol, Any}(pairs(key))
    for (cond, belief_types) in conditions
        # Compute overall correlation for condition
        sub_df = filter(r -> r.belief_type in belief_types, group)
        m_statement_probs = sub_df.statement_probs

        h_statement_probs = vec(mean_human_statement_probs')[sub_df.rating_id]
        row[Symbol("$(cond)_cor")] = cor(h_statement_probs, m_statement_probs)

        h_sampled_statement_probs = [vec(probs')[sub_df.rating_id] for probs in sampled_statement_probs]
        sampled_cors = [cor(probs, m_statement_probs) for probs in h_sampled_statement_probs]
        row[Symbol("$(cond)_cor_se")] = std(sampled_cors)
        row[Symbol("$(cond)_cor_ci_lo")] = quantile(sampled_cors, 0.025)
        row[Symbol("$(cond)_cor_ci_hi")] = quantile(sampled_cors, 0.975)

        # Iterate over factors
        for factor in factors
            sub_df = filter(r -> r.belief_type in belief_types && r[factor] == 1, group)
            m_statement_probs = sub_df.statement_probs

            h_statement_probs = vec(mean_human_statement_probs')[sub_df.rating_id]
            row[Symbol("$(cond)_$(factor)_cor")] = cor(h_statement_probs, m_statement_probs)

            h_sampled_statement_probs = [vec(probs')[sub_df.rating_id] for probs in sampled_statement_probs]
            row[Symbol("$(cond)_$(factor)_cor_se")] = std(cor(probs, m_statement_probs) for probs in h_sampled_statement_probs)
        end
    end

    push!(statement_correlation_df, row, cols=:union)
end

CSV.write(joinpath(RESULTS_DIR, "human_model_corr_per_factor.csv"), statement_correlation_df)

# Format correlation table for publication
formatter = (xs, ses) -> begin
    [ismissing(x) ? missing : @sprintf("%.2f (%.2f)", x, se) for (x, se) in zip(xs, ses)]
end

formatted_df = DataFrames.select(statement_correlation_df,
    :method => :method,
    :submethod => :submethod,
    [:all_cor, :all_cor_se] => formatter => :all_cor,
    [:curr_cor, :curr_cor_se] => formatter => :curr_cor,
    [:init_cor, :init_cor_se] => formatter => :init_cor,
    [:all_possibility_cor, :all_possibility_cor_se] => formatter => :possibility_cor,
    [:all_probability_cor, :all_probability_cor_se] => formatter => :probability_cor,
    [:all_compositional_cor, :all_compositional_cor_se] => formatter => :compositional_cor,
    [:all_knowledge_cor, :all_knowledge_cor_se] => formatter => :knowledge_cor,
    [:curr_possibility_cor, :curr_possibility_cor_se] => formatter => :curr_possibility_cor,
    [:curr_probability_cor, :curr_probability_cor_se] => formatter => :curr_probability_cor,
    [:curr_compositional_cor, :curr_compositional_cor_se] => formatter => :curr_compositional_cor,
    [:curr_knowledge_cor, :curr_knowledge_cor_se] => formatter => :curr_knowledge_cor,
    [:init_possibility_cor, :init_possibility_cor_se] => formatter => :init_possibility_cor,
    [:init_probability_cor, :init_probability_cor_se] => formatter => :init_probability_cor,
    [:init_compositional_cor, :init_compositional_cor_se] => formatter => :init_compositional_cor,
    [:init_knowledge_cor, :init_knowledge_cor_se] => formatter => :init_knowledge_cor
)

display(formatted_df)
CSV.write(joinpath(RESULTS_DIR, "human_model_corr_per_factor_formatted.csv"), formatted_df)

## Analyze likelihood differences for full dataset ##

using HypothesisTests

# Load all results
RESULTS_DIR = joinpath(@__DIR__, "results")
LIKELIHOOD_DF_PATHS = [
    ("btom", "full", "current") => joinpath(RESULTS_DIR, "exp1", "exp1_current_btom.csv"),
    ("btom", "full", "initial") => joinpath(RESULTS_DIR, "exp1", "exp1_initial_btom.csv"),
    ("btom", "non_instrumental", "current") => joinpath(RESULTS_DIR, "exp1", "exp1_current_non_instrumental.csv"),
    ("btom", "non_instrumental", "initial") => joinpath(RESULTS_DIR, "exp1", "exp1_initial_non_instrumental.csv"),
    ("btom", "true_belief", "current") => joinpath(RESULTS_DIR, "exp1", "exp1_current_true_belief.csv"),
    ("btom", "true_belief", "initial") => joinpath(RESULTS_DIR, "exp1", "exp1_initial_true_belief.csv"),
    ("gpt-4o", "image, narrative", "current") => joinpath(RESULTS_DIR, "exp1","exp1_current_gpt4o.csv"),
    ("gpt-4o", "image, narrative", "initial") => joinpath(RESULTS_DIR, "exp1","exp1_initial_gpt4o.csv"),
]
START_COLS = [:method, :submethod, :belief_type]

likelihood_df = DataFrame()
for ((method, submethod, belief_type), path) in LIKELIHOOD_DF_PATHS
    df = CSV.read(path, DataFrame)
    df.method .= method
    df.submethod .= submethod
    df.belief_type .= belief_type
    if method != "btom"
        df.norm_statement_probs .= df.statement_probs
    end
    select!(df, START_COLS, Not(START_COLS))
    append!(likelihood_df, df, cols=:union)
end

# Compute likelihood differences across full dataset per method and belief type
gdf = DataFrames.groupby(likelihood_df, [:method, :submethod, :belief_type])
likelihood_per_method_df = combine(gdf,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[is])) => :in_score,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[.!is])) => :out_score,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[is]) - mean(xs[.!is])) => :score_diff,
    [:match, :norm_statement_probs] => ((is, xs) -> sem(xs[.!is])) => :in_score_se,
    [:match, :norm_statement_probs] => ((is, xs) -> sem(xs[.!is])) => :out_score_se,
    [:match, :norm_statement_probs] => ((is, xs) -> sqrt(sem(xs[is])^2 + sem(xs[.!is])^2)) => :score_diff_se
)
CSV.write(joinpath(RESULTS_DIR, "likelihood_per_method.csv"), likelihood_per_method_df)

# Compute likelihood differences per map
gdf = DataFrames.groupby(likelihood_df, [:method, :submethod, :belief_type, :map_id])
likelihood_per_map_df = combine(gdf,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[is])) => :in_score,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[.!is])) => :out_score,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[is]) - mean(xs[.!is])) => :score_diff,
    [:match, :norm_statement_probs] => ((is, xs) -> sem(xs[.!is])) => :in_score_se,
    [:match, :norm_statement_probs] => ((is, xs) -> sem(xs[.!is])) => :out_score_se,
    [:match, :norm_statement_probs] => ((is, xs) -> sqrt(sem(xs[is])^2 + sem(xs[.!is])^2)) => :score_diff_se
)

# Compute likelihood differences per plan
gdf = DataFrames.groupby(likelihood_df, [:method, :submethod, :belief_type, :plan_id])
likelihood_per_plan_df = combine(gdf,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[is])) => :in_score,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[.!is])) => :out_score,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[is]) - mean(xs[.!is])) => :score_diff,
    [:match, :norm_statement_probs] => ((is, xs) -> sem(xs[.!is])) => :in_score_se,
    [:match, :norm_statement_probs] => ((is, xs) -> sem(xs[.!is])) => :out_score_se,
    [:match, :norm_statement_probs] => ((is, xs) -> sqrt(sem(xs[is])^2 + sem(xs[.!is])^2)) => :score_diff_se
)

# Compute likelihood differences per statement
gdf = DataFrames.groupby(likelihood_df, [:method, :submethod, :belief_type, :statement_id])
likelihood_per_statement_df = combine(gdf,
    :map_id => first => :map_id,
    :source_id => first => :in_plan_id,
    [:match, :plan_id] => ((is, xs) -> join(unique(xs[.!is]), "; ")) => :out_plan_ids,
    :statement => first => :statement,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[is])) => :in_score,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[.!is])) => :out_score,
    [:match, :norm_statement_probs] => ((is, xs) -> mean(xs[is]) - mean(xs[.!is])) => :score_diff
)
transform!(likelihood_per_statement_df,
    :score_diff => (x -> (x .> 0) .* 1.0) => :correct
)
CSV.write(joinpath(RESULTS_DIR, "likelihood_per_statement.csv"), likelihood_per_statement_df)

# Compute difference of differences
gdf = DataFrames.groupby(likelihood_per_statement_df, [:method, :submethod, :belief_type])
for (key, group) in pairs(gdf)
    ref = gdf[(method="btom", submethod="full", belief_type=key.belief_type)]
    group.diff_diff .= group.score_diff .- ref.score_diff
    group.correct_diff .= group.correct .- ref.correct
end

# Compute mean accuracy across statements per method and belief type
gdf = DataFrames.groupby(likelihood_per_statement_df, [:method, :submethod, :belief_type])
likelihood_performance_df = combine(gdf,
    :in_score => mean => :in_score,
    :in_score => sem => :in_score_se,
    :out_score => mean => :out_score,
    :out_score => sem => :out_score_se,
    :score_diff => mean => :score_diff,
    :score_diff => sem => :score_diff_se,
    :correct => mean => :accuracy,
    :correct => sem => :accuracy_se,
    :diff_diff => (xs -> pvalue(OneSampleTTest(float.(xs)), tail=:left)) => :diff_diff_pval,
    :correct_diff => (xs -> pvalue(OneSampleTTest(float.(xs)), tail=:left)) => :accuracy_diff_pval
)
CSV.write(joinpath(RESULTS_DIR, "likelihood_performance.csv"), likelihood_performance_df)

# Format results for paper
formatter_score = (in, out, diff, diff_se) -> begin
   [@sprintf("%.2f / %.2f / %+.2f (%.2f)", i, o, d, ds) for (i, o, d, ds) in zip(in, out, diff, diff_se)]
end
formatter_acc = (xs, ses) -> begin
    [ismissing(x) ? missing : @sprintf("%.2f (%.2f)", x, se) for (x, se) in zip(xs, ses)]
end
formatted_df = DataFrames.select(likelihood_performance_df,
    :method => :method,
    :submethod => :submethod,
    :belief_type => :belief_type,
    [:in_score, :out_score, :score_diff, :score_diff_se] => formatter_score => :scores,
    [:accuracy, :accuracy_se] => formatter_acc => :accuracy,
)
sort!(formatted_df, [:belief_type, :method, :submethod])

CSV.write(joinpath(RESULTS_DIR, "likelihood_performance_formatted.csv"), formatted_df)
